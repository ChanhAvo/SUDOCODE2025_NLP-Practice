{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:12:38.224260Z",
     "iopub.status.busy": "2025-10-14T09:12:38.223691Z",
     "iopub.status.idle": "2025-10-14T09:14:05.410517Z",
     "shell.execute_reply": "2025-10-14T09:14:05.409548Z",
     "shell.execute_reply.started": "2025-10-14T09:12:38.224229Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers==4.44.2 datasets sentencepiece evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:14:05.412389Z",
     "iopub.status.busy": "2025-10-14T09:14:05.412143Z",
     "iopub.status.idle": "2025-10-14T09:14:10.590298Z",
     "shell.execute_reply": "2025-10-14T09:14:10.589281Z",
     "shell.execute_reply.started": "2025-10-14T09:14:05.412368Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:14:10.591783Z",
     "iopub.status.busy": "2025-10-14T09:14:10.591476Z",
     "iopub.status.idle": "2025-10-14T09:14:35.757666Z",
     "shell.execute_reply": "2025-10-14T09:14:35.757038Z",
     "shell.execute_reply.started": "2025-10-14T09:14:10.591749Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 09:14:17.581023: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760433257.844362      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760433257.921856      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f9fc007ad348749b429fb7fe6fd1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,DataCollatorForSeq2Seq,Seq2SeqTrainingArguments,Seq2SeqTrainer\n",
    "import evaluate\n",
    "import nltk\n",
    "import evaluate\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Methodology**\n",
    "\n",
    "- This project implements abstractive text summarization using the **T5-base** model. It's an encoder-decoder model where attention is used in multiple ways: the encoder uses self-attention to process the input, the decoder uses self-attention on the output, and cross-attention is used for the decoder to attend to the encoder's output. The model is fine-tuned on the **CNN/DailyMail dataset**. For computational efficiency, 20,000 samples from the training set and 2,000 each from validation and test sets were used for fine-tuning and evaluation.\n",
    "- Evaluation metrics: The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric was used for quantitative assessment, including:\n",
    "    - **ROUGE-1:** Overlap of unigrams (word-level)\n",
    "    - **ROUGE-2:** Overlap of bigrams\n",
    "    - **ROUGE-L / ROUGE-Lsum:** Longest common subsequence (measures fluency and structure)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:14:35.759981Z",
     "iopub.status.busy": "2025-10-14T09:14:35.759457Z",
     "iopub.status.idle": "2025-10-14T09:15:04.794490Z",
     "shell.execute_reply": "2025-10-14T09:15:04.793875Z",
     "shell.execute_reply.started": "2025-10-14T09:14:35.759962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n",
       "      <td>A drunk driver who killed a young woman in a h...</td>\n",
       "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
       "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
       "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n",
       "      <td>Fleetwood are the only team still to have a 10...</td>\n",
       "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
       "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
       "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
       "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
       "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
       "\n",
       "                                             article  \\\n",
       "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
       "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "2  A drunk driver who killed a young woman in a h...   \n",
       "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
       "4  Fleetwood are the only team still to have a 10...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Bishop John Folda, of North Dakota, is taking ...  \n",
       "1  Criminal complaint: Cop used his role to help ...  \n",
       "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
       "3  Nina dos Santos says Europe must be ready to a...  \n",
       "4  Fleetwood top of League One after 2-0 win at S...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv')\n",
    "train_df = pd.DataFrame(train)\n",
    "val = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv')\n",
    "val_df = pd.DataFrame(val)\n",
    "test = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv')\n",
    "test_df = pd.DataFrame(test)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:15:04.795276Z",
     "iopub.status.busy": "2025-10-14T09:15:04.795103Z",
     "iopub.status.idle": "2025-10-14T09:15:04.908901Z",
     "shell.execute_reply": "2025-10-14T09:15:04.908246Z",
     "shell.execute_reply.started": "2025-10-14T09:15:04.795262Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 287113 entries, 0 to 287112\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   id          287113 non-null  object\n",
      " 1   article     287113 non-null  object\n",
      " 2   highlights  287113 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:15:04.909907Z",
     "iopub.status.busy": "2025-10-14T09:15:04.909603Z",
     "iopub.status.idle": "2025-10-14T09:15:04.969338Z",
     "shell.execute_reply": "2025-10-14T09:15:04.968781Z",
     "shell.execute_reply.started": "2025-10-14T09:15:04.909890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13368 entries, 0 to 13367\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          13368 non-null  object\n",
      " 1   article     13368 non-null  object\n",
      " 2   highlights  13368 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 313.4+ KB\n"
     ]
    }
   ],
   "source": [
    "val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:15:04.970436Z",
     "iopub.status.busy": "2025-10-14T09:15:04.970164Z",
     "iopub.status.idle": "2025-10-14T09:15:04.987096Z",
     "shell.execute_reply": "2025-10-14T09:15:04.986447Z",
     "shell.execute_reply.started": "2025-10-14T09:15:04.970418Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11490 entries, 0 to 11489\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          11490 non-null  object\n",
      " 1   article     11490 non-null  object\n",
      " 2   highlights  11490 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 269.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:15:04.988700Z",
     "iopub.status.busy": "2025-10-14T09:15:04.988036Z",
     "iopub.status.idle": "2025-10-14T09:15:05.020258Z",
     "shell.execute_reply": "2025-10-14T09:15:05.019572Z",
     "shell.execute_reply.started": "2025-10-14T09:15:04.988655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Drop unused columns \n",
    "train_df = train_df.drop(columns = ['id'])\n",
    "val_df = val_df.drop(columns = ['id'])\n",
    "test_df = test_df.drop(columns = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:15:05.021188Z",
     "iopub.status.busy": "2025-10-14T09:15:05.020966Z",
     "iopub.status.idle": "2025-10-14T09:15:09.187665Z",
     "shell.execute_reply": "2025-10-14T09:15:09.187001Z",
     "shell.execute_reply.started": "2025-10-14T09:15:05.021169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train duplicates:  3098\n",
      "Validation duplicates:  0\n",
      "Test duplicates:  2\n"
     ]
    }
   ],
   "source": [
    "#Check duplicates \n",
    "print(\"Train duplicates: \", train_df.duplicated().sum())\n",
    "print(\"Validation duplicates: \", val_df.duplicated().sum())\n",
    "print(\"Test duplicates: \", test_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:15:09.190048Z",
     "iopub.status.busy": "2025-10-14T09:15:09.189776Z",
     "iopub.status.idle": "2025-10-14T09:15:11.979608Z",
     "shell.execute_reply": "2025-10-14T09:15:11.978974Z",
     "shell.execute_reply.started": "2025-10-14T09:15:09.190031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Drop duplicates \n",
    "train_df = train_df.drop_duplicates()\n",
    "test_df = test_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:15:11.980471Z",
     "iopub.status.busy": "2025-10-14T09:15:11.980226Z",
     "iopub.status.idle": "2025-10-14T09:15:12.001792Z",
     "shell.execute_reply": "2025-10-14T09:15:12.001158Z",
     "shell.execute_reply.started": "2025-10-14T09:15:11.980446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (20000, 2), Val: (2000, 2), Test: (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "#Sampling the dataset for faster training \n",
    "training_set = train_df.sample(n=20000, random_state=42)\n",
    "validation_set = val_df.sample(n=2000, random_state=42)\n",
    "test_set = test_df.sample(n=2000, random_state=42)\n",
    "print(f\"Train: {training_set.shape}, Val: {validation_set.shape}, Test: {test_set.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:15:12.002696Z",
     "iopub.status.busy": "2025-10-14T09:15:12.002468Z",
     "iopub.status.idle": "2025-10-14T09:15:17.506761Z",
     "shell.execute_reply": "2025-10-14T09:15:17.506132Z",
     "shell.execute_reply.started": "2025-10-14T09:15:12.002681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Clean text \n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "training_set['article'] = training_set['article'].apply(clean_text)\n",
    "training_set['highlights'] = training_set['highlights'].apply(clean_text)\n",
    "\n",
    "validation_set['article'] = validation_set['article'].apply(clean_text)\n",
    "validation_set['highlights'] = validation_set['highlights'].apply(clean_text)\n",
    "\n",
    "test_set['article'] = test_set['article'].apply(clean_text)\n",
    "test_set['highlights'] = test_set['highlights'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:15:17.507747Z",
     "iopub.status.busy": "2025-10-14T09:15:17.507522Z",
     "iopub.status.idle": "2025-10-14T09:15:18.332097Z",
     "shell.execute_reply": "2025-10-14T09:15:18.331336Z",
     "shell.execute_reply.started": "2025-10-14T09:15:17.507732Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(training_set)\n",
    "val_dataset = Dataset.from_pandas(validation_set)\n",
    "test_dataset = Dataset.from_pandas(test_set)\n",
    "for ds in [train_dataset, val_dataset, test_dataset]:\n",
    "    if \"__index_level_0__\" in ds.column_names:\n",
    "        ds = ds.remove_columns([\"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:15:18.333143Z",
     "iopub.status.busy": "2025-10-14T09:15:18.332889Z",
     "iopub.status.idle": "2025-10-14T09:15:23.010152Z",
     "shell.execute_reply": "2025-10-14T09:15:23.009314Z",
     "shell.execute_reply.started": "2025-10-14T09:15:18.333121Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37e01874c824dccbde9c7cd93b56454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89049f23d3924eddb5e3ba19a915d1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f52ffe700b64d178f71ed16c6ddf8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3246de57c5ab48529f20bd966afdc853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3447c65683e1442e9765bf26cf50c619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Prepare model \n",
    "model_name = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:15:23.011431Z",
     "iopub.status.busy": "2025-10-14T09:15:23.011164Z",
     "iopub.status.idle": "2025-10-14T09:15:59.716982Z",
     "shell.execute_reply": "2025-10-14T09:15:59.716204Z",
     "shell.execute_reply.started": "2025-10-14T09:15:23.011390Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441ea9752fbd4159851c8e0c79bd3f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada9b2a8754c482f97a9885a40365b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tokenization \n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess (examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]\n",
    "    model_input = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(examples[\"highlights\"], max_length=max_target_length, truncation=True)\n",
    "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_input\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess, batched=True)\n",
    "tokenized_val   = val_dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:15:59.718470Z",
     "iopub.status.busy": "2025-10-14T09:15:59.717851Z",
     "iopub.status.idle": "2025-10-14T09:16:02.758270Z",
     "shell.execute_reply": "2025-10-14T09:16:02.757547Z",
     "shell.execute_reply.started": "2025-10-14T09:15:59.718443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e4328f701340dc926c38b8ba06854d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_test = test_dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:16:02.759156Z",
     "iopub.status.busy": "2025-10-14T09:16:02.758960Z",
     "iopub.status.idle": "2025-10-14T09:16:02.762681Z",
     "shell.execute_reply": "2025-10-14T09:16:02.761910Z",
     "shell.execute_reply.started": "2025-10-14T09:16:02.759140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:16:02.763660Z",
     "iopub.status.busy": "2025-10-14T09:16:02.763442Z",
     "iopub.status.idle": "2025-10-14T09:16:03.298960Z",
     "shell.execute_reply": "2025-10-14T09:16:03.298373Z",
     "shell.execute_reply.started": "2025-10-14T09:16:02.763645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Metrics (ROUGE)\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:16:03.300069Z",
     "iopub.status.busy": "2025-10-14T09:16:03.299793Z",
     "iopub.status.idle": "2025-10-14T09:16:04.248319Z",
     "shell.execute_reply": "2025-10-14T09:16:04.247774Z",
     "shell.execute_reply.started": "2025-10-14T09:16:03.300047Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "#Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-summarizer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=1,  \n",
    "    predict_with_generate=True,\n",
    "    fp16=True,  \n",
    "    push_to_hub=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    ")\n",
    "\n",
    "#Trainer setup\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:16:04.249160Z",
     "iopub.status.busy": "2025-10-14T09:16:04.248984Z",
     "iopub.status.idle": "2025-10-14T09:59:07.536542Z",
     "shell.execute_reply": "2025-10-14T09:59:07.535943Z",
     "shell.execute_reply.started": "2025-10-14T09:16:04.249147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 43:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.625200</td>\n",
       "      <td>1.554892</td>\n",
       "      <td>25.204000</td>\n",
       "      <td>12.546700</td>\n",
       "      <td>21.121100</td>\n",
       "      <td>21.114000</td>\n",
       "      <td>1899.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=1.644291845703125, metrics={'train_runtime': 2582.8564, 'train_samples_per_second': 7.743, 'train_steps_per_second': 0.968, 'total_flos': 1.21791578112e+16, 'train_loss': 1.644291845703125, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T10:05:02.666252Z",
     "iopub.status.busy": "2025-10-14T10:05:02.665966Z",
     "iopub.status.idle": "2025-10-14T10:09:24.775366Z",
     "shell.execute_reply": "2025-10-14T10:09:24.774759Z",
     "shell.execute_reply.started": "2025-10-14T10:05:02.666232Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 04:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5548917055130005, 'eval_rouge1': 25.204, 'eval_rouge2': 12.5467, 'eval_rougeL': 21.1211, 'eval_rougeLsum': 21.114, 'eval_gen_len': 1899.9, 'eval_runtime': 262.0986, 'eval_samples_per_second': 7.631, 'eval_steps_per_second': 0.954, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "#Evaluate on validation set\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation results indicate that after one epoch of training, the model achieved a validation loss of 1.55, suggesting it has learned to generate summaries with moderate accuracy but still has room for improvement. The **ROUGE-1** score of **25.20%** and **ROUGE-L** score of **21.12%** show that the model captures some key words and phrases from the reference summaries. \n",
    "\n",
    "Notably, the average generated length (eval_gen_len) is around **1900 tokens**, which is far longer than the intended summary length and indicates a configuration issue with generation parameters â€” the model likely produced excessively long outputs instead of concise summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T12:40:02.540317Z",
     "iopub.status.busy": "2025-10-14T12:40:02.539732Z",
     "iopub.status.idle": "2025-10-14T12:40:04.458984Z",
     "shell.execute_reply": "2025-10-14T12:40:04.458318Z",
     "shell.execute_reply.started": "2025-10-14T12:40:02.540294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Article:\n",
      " Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by t ...\n",
      "\n",
      "Generated Summary:\n",
      " The FAA conducts tests on how quickly passengers can leave a plane, but these tests are conducted using planes with a 31 inch pitch. While most airlines stick to a pitch of 31 inches or above, some airlines fall below this. British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches and Virgin Atlantic's is 30-31 inches.\n"
     ]
    }
   ],
   "source": [
    "sample_text = test_df[\"article\"].iloc[0]\n",
    "inputs = tokenizer(\"summarize: \" + sample_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=128, early_stopping=True)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nOriginal Article:\\n\", sample_text[:500], \"...\")\n",
    "print(\"\\nGenerated Summary:\\n\", tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1654566,
     "sourceId": 2734496,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

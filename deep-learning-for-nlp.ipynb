{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Set up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T13:45:14.153843Z",
     "iopub.status.busy": "2025-10-11T13:45:14.153597Z",
     "iopub.status.idle": "2025-10-11T13:45:26.508954Z",
     "shell.execute_reply": "2025-10-11T13:45:26.507733Z",
     "shell.execute_reply.started": "2025-10-11T13:45:14.153820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting underthesea\n",
      "  Downloading underthesea-8.3.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from underthesea) (8.3.0)\n",
      "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
      "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in /usr/local/lib/python3.11/dist-packages (from underthesea) (3.9.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from underthesea) (4.67.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from underthesea) (2.32.5)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.5.2)\n",
      "Collecting scikit-learn>=1.6.1 (from underthesea)\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from underthesea) (6.0.3)\n",
      "Collecting underthesea_core==1.0.5 (from underthesea)\n",
      "  Downloading underthesea_core-1.0.5-cp311-cp311-manylinux2010_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.0.0rc2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.8->underthesea) (2025.9.18)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.1->underthesea) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.1->underthesea) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.1->underthesea) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->underthesea) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->underthesea) (2025.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->underthesea) (25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->underthesea) (0.28.1)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->underthesea) (0.19.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->underthesea) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->underthesea) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2025.8.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub->underthesea) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub->underthesea) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub->underthesea) (0.16.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn>=1.6.1->underthesea) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn>=1.6.1->underthesea) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn>=1.6.1->underthesea) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn>=1.6.1->underthesea) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn>=1.6.1->underthesea) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn>=1.6.1->underthesea) (2.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub->underthesea) (1.3.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn>=1.6.1->underthesea) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn>=1.6.1->underthesea) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.0->scikit-learn>=1.6.1->underthesea) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.0->scikit-learn>=1.6.1->underthesea) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.0->scikit-learn>=1.6.1->underthesea) (2024.2.0)\n",
      "Downloading underthesea-8.3.0-py3-none-any.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading underthesea_core-1.0.5-cp311-cp311-manylinux2010_x86_64.whl (978 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.6/978.6 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: underthesea_core, python-crfsuite, scikit-learn, underthesea\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed python-crfsuite-0.9.11 scikit-learn-1.7.2 underthesea-8.3.0 underthesea_core-1.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T13:56:56.542438Z",
     "iopub.status.busy": "2025-10-11T13:56:56.541580Z",
     "iopub.status.idle": "2025-10-11T13:56:56.549051Z",
     "shell.execute_reply": "2025-10-11T13:56:56.547939Z",
     "shell.execute_reply.started": "2025-10-11T13:56:56.542406Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T13:57:01.237072Z",
     "iopub.status.busy": "2025-10-11T13:57:01.236715Z",
     "iopub.status.idle": "2025-10-11T13:57:12.626119Z",
     "shell.execute_reply": "2025-10-11T13:57:12.624987Z",
     "shell.execute_reply.started": "2025-10-11T13:57:01.237043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'VNTC'...\n",
      "remote: Enumerating objects: 39, done.\u001b[K\n",
      "remote: Total 39 (delta 0), reused 0 (delta 0), pack-reused 39 (from 1)\u001b[K\n",
      "Receiving objects: 100% (39/39), 160.90 MiB | 38.92 MiB/s, done.\n",
      "Resolving deltas: 100% (4/4), done.\n",
      "Updating files: 100% (15/15), done.\n",
      "Filtering content: 100% (2/2), 168.95 MiB | 42.70 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/duyvuleo/VNTC.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T13:57:33.487012Z",
     "iopub.status.busy": "2025-10-11T13:57:33.486584Z",
     "iopub.status.idle": "2025-10-11T13:57:45.591073Z",
     "shell.execute_reply": "2025-10-11T13:57:45.589734Z",
     "shell.execute_reply.started": "2025-10-11T13:57:33.486980Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed!\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /kaggle/working/VNTC/Data/10Topics/Ver1.1/train\n",
    "!mkdir -p /kaggle/working/VNTC/Data/10Topics/Ver1.1/test\n",
    "\n",
    "!unrar x /kaggle/working/VNTC/Data/10Topics/Ver1.1/Train_Full.rar /kaggle/working/VNTC/Data/10Topics/Ver1.1/train/ > /dev/null 2>&1\n",
    "!unrar x /kaggle/working/VNTC/Data/10Topics/Ver1.1/Test_Full.rar /kaggle/working/VNTC/Data/10Topics/Ver1.1/test/ > /dev/null 2>&1\n",
    "print(\"Extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T13:57:53.161253Z",
     "iopub.status.busy": "2025-10-11T13:57:53.160838Z",
     "iopub.status.idle": "2025-10-11T13:57:53.169599Z",
     "shell.execute_reply": "2025-10-11T13:57:53.168304Z",
     "shell.execute_reply.started": "2025-10-11T13:57:53.161206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_vntc_data(base_dir):\n",
    "    texts, labels = [], []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for fname in files:\n",
    "            if fname.endswith('.txt'):\n",
    "                fpath = os.path.join(root, fname)\n",
    "                label = os.path.basename(os.path.dirname(fpath))\n",
    "    \n",
    "                try:\n",
    "                    with open(fpath, encoding='utf-8') as f:\n",
    "                        text = f.read()\n",
    "                except UnicodeDecodeError:\n",
    "                    try:\n",
    "                        with open(fpath, encoding='utf-16') as f:\n",
    "                            text = f.read()\n",
    "                    except UnicodeDecodeError:\n",
    "                        with open(fpath, encoding='latin-1') as f:\n",
    "                            text = f.read()\n",
    "                \n",
    "                texts.append(text)\n",
    "                labels.append(label)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T13:57:55.521402Z",
     "iopub.status.busy": "2025-10-11T13:57:55.521049Z",
     "iopub.status.idle": "2025-10-11T13:58:02.214902Z",
     "shell.execute_reply": "2025-10-11T13:58:02.213849Z",
     "shell.execute_reply.started": "2025-10-11T13:57:55.521376Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 33759 samples, 10 topics.\n",
      "Loaded 50373 samples, 10 topics.\n"
     ]
    }
   ],
   "source": [
    "train_texts, train_labels = load_vntc_data(\"/kaggle/working/VNTC/Data/10Topics/Ver1.1/train/Train_Full\")\n",
    "test_texts, test_labels = load_vntc_data(\"/kaggle/working/VNTC/Data/10Topics/Ver1.1/test/Test_Full\")\n",
    "print(f\"Loaded {len(train_texts)} samples, {len(set(train_labels))} topics.\")\n",
    "print(f\"Loaded {len(test_texts)} samples, {len(set(test_labels))} topics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T13:58:05.376278Z",
     "iopub.status.busy": "2025-10-11T13:58:05.375902Z",
     "iopub.status.idle": "2025-10-11T13:58:05.508013Z",
     "shell.execute_reply": "2025-10-11T13:58:05.507146Z",
     "shell.execute_reply.started": "2025-10-11T13:58:05.376245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Stratify train set and test set for faster preprocessing and training \n",
    "train_texts_small, _, train_labels_small, _ = train_test_split(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    train_size=2000,\n",
    "    stratify=train_labels,   \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "test_texts_small, _, test_labels_small, _ = train_test_split(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    train_size=2000,\n",
    "    stratify=test_labels,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T13:58:32.786293Z",
     "iopub.status.busy": "2025-10-11T13:58:32.785934Z",
     "iopub.status.idle": "2025-10-11T13:58:32.809869Z",
     "shell.execute_reply": "2025-10-11T13:58:32.808684Z",
     "shell.execute_reply.started": "2025-10-11T13:58:32.786267Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution:\n",
      "The thao            0.1570\n",
      "Chinh tri Xa hoi    0.1545\n",
      "Phap luat           0.1145\n",
      "Suc khoe            0.1005\n",
      "Doi song            0.0935\n",
      "Van hoa             0.0910\n",
      "The gioi            0.0860\n",
      "Kinh doanh          0.0755\n",
      "Vi tinh             0.0735\n",
      "Khoa hoc            0.0540\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test class distribution:\n",
      "Chinh tri Xa hoi    0.1500\n",
      "The gioi            0.1335\n",
      "The thao            0.1325\n",
      "Van hoa             0.1240\n",
      "Suc khoe            0.1075\n",
      "Kinh doanh          0.1050\n",
      "Vi tinh             0.0905\n",
      "Phap luat           0.0750\n",
      "Khoa hoc            0.0415\n",
      "Doi song            0.0405\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Inspect stratified data\n",
    "print(\"Train class distribution:\")\n",
    "print(pd.Series(train_labels_small).value_counts(normalize=True))\n",
    "print(\"\\nTest class distribution:\")\n",
    "print(pd.Series(test_labels_small).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T13:58:35.417808Z",
     "iopub.status.busy": "2025-10-11T13:58:35.416943Z",
     "iopub.status.idle": "2025-10-11T13:58:35.427014Z",
     "shell.execute_reply": "2025-10-11T13:58:35.425969Z",
     "shell.execute_reply.started": "2025-10-11T13:58:35.417779Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Chinh tri Xa hoi' 'Doi song' 'Khoa hoc' 'Kinh doanh' 'Phap luat'\n",
      " 'Suc khoe' 'The gioi' 'The thao' 'Van hoa' 'Vi tinh']\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_labels_small)\n",
    "y_test = le.transform(test_labels_small)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Classes:\", le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T13:58:37.768347Z",
     "iopub.status.busy": "2025-10-11T13:58:37.767979Z",
     "iopub.status.idle": "2025-10-11T13:58:37.774313Z",
     "shell.execute_reply": "2025-10-11T13:58:37.773372Z",
     "shell.execute_reply.started": "2025-10-11T13:58:37.768321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess (text):\n",
    "    #Lowercase \n",
    "    text = text.lower()\n",
    "    #Remove url \n",
    "    text = re.sub(r\"https?://\\S+\", '',text)\n",
    "    #Remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-ZÀ-ỹ\\s]\", \" \", text)\n",
    "    #Strip extra space \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    #Tokenize \n",
    "    text = word_tokenize(text, format=\"text\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T13:58:40.076090Z",
     "iopub.status.busy": "2025-10-11T13:58:40.075750Z",
     "iopub.status.idle": "2025-10-11T14:01:07.586477Z",
     "shell.execute_reply": "2025-10-11T14:01:07.585548Z",
     "shell.execute_reply.started": "2025-10-11T13:58:40.076066Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_texts_clean = [preprocess(t) for t in train_texts_small]\n",
    "test_texts_clean = [preprocess(t) for t in test_texts_small]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T14:02:19.175671Z",
     "iopub.status.busy": "2025-10-11T14:02:19.174679Z",
     "iopub.status.idle": "2025-10-11T14:02:19.185910Z",
     "shell.execute_reply": "2025-10-11T14:02:19.184687Z",
     "shell.execute_reply.started": "2025-10-11T14:02:19.175641Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1600, Val: 400, Test: 2000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_texts_clean, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(test_texts_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T14:02:22.254283Z",
     "iopub.status.busy": "2025-10-11T14:02:22.253334Z",
     "iopub.status.idle": "2025-10-11T14:02:23.611562Z",
     "shell.execute_reply": "2025-10-11T14:02:23.610469Z",
     "shell.execute_reply.started": "2025-10-11T14:02:22.254240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#TF-IDF \n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_test_tfidf = vectorizer.transform(test_texts_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Building model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T14:02:27.429377Z",
     "iopub.status.busy": "2025-10-11T14:02:27.429004Z",
     "iopub.status.idle": "2025-10-11T14:02:28.293497Z",
     "shell.execute_reply": "2025-10-11T14:02:28.290118Z",
     "shell.execute_reply.started": "2025-10-11T14:02:27.429352Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.845\n"
     ]
    }
   ],
   "source": [
    "#Baseline model check (Logistic Regression)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print(\"Validation Accuracy:\", clf.score(X_val_tfidf, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T14:02:34.175107Z",
     "iopub.status.busy": "2025-10-11T14:02:34.174001Z",
     "iopub.status.idle": "2025-10-11T14:02:34.406541Z",
     "shell.execute_reply": "2025-10-11T14:02:34.405671Z",
     "shell.execute_reply.started": "2025-10-11T14:02:34.175055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TfidfDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.toarray(), dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TfidfDataset(X_train_tfidf, y_train)\n",
    "val_dataset = TfidfDataset(X_val_tfidf, y_val)\n",
    "test_dataset = TfidfDataset(X_test_tfidf, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T14:02:36.765999Z",
     "iopub.status.busy": "2025-10-11T14:02:36.765622Z",
     "iopub.status.idle": "2025-10-11T14:02:36.801604Z",
     "shell.execute_reply": "2025-10-11T14:02:36.800338Z",
     "shell.execute_reply.started": "2025-10-11T14:02:36.765975Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#Model architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SimpleNN(input_dim=3000, hidden_dim=512, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T14:02:39.897091Z",
     "iopub.status.busy": "2025-10-11T14:02:39.896179Z",
     "iopub.status.idle": "2025-10-11T14:02:44.770462Z",
     "shell.execute_reply": "2025-10-11T14:02:44.769573Z",
     "shell.execute_reply.started": "2025-10-11T14:02:39.897058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Loss: 23.850 | Val Acc: 0.8450\n",
      "Epoch 2/15 | Loss: 3.202 | Val Acc: 0.8500\n",
      "Epoch 3/15 | Loss: 0.988 | Val Acc: 0.8425\n",
      "Epoch 4/15 | Loss: 0.447 | Val Acc: 0.8350\n",
      "Epoch 5/15 | Loss: 0.281 | Val Acc: 0.8425\n",
      "Epoch 6/15 | Loss: 0.188 | Val Acc: 0.8350\n",
      "Epoch 7/15 | Loss: 0.149 | Val Acc: 0.8500\n",
      "Epoch 8/15 | Loss: 0.117 | Val Acc: 0.8375\n",
      "Epoch 9/15 | Loss: 0.096 | Val Acc: 0.8375\n",
      "Epoch 10/15 | Loss: 0.081 | Val Acc: 0.8475\n",
      "Epoch 11/15 | Loss: 0.070 | Val Acc: 0.8400\n",
      "Epoch 12/15 | Loss: 0.064 | Val Acc: 0.8425\n",
      "Epoch 13/15 | Loss: 0.052 | Val Acc: 0.8425\n",
      "Epoch 14/15 | Loss: 0.046 | Val Acc: 0.8450\n",
      "Epoch 15/15 | Loss: 0.040 | Val Acc: 0.8475\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    acc = correct / total\n",
    "    return acc, all_preds, all_labels\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 15  \n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    val_acc, _, _ = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.3f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T14:02:49.611298Z",
     "iopub.status.busy": "2025-10-11T14:02:49.610937Z",
     "iopub.status.idle": "2025-10-11T14:02:49.759035Z",
     "shell.execute_reply": "2025-10-11T14:02:49.758061Z",
     "shell.execute_reply.started": "2025-10-11T14:02:49.611273Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8830\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       300\n",
      "           1       0.73      0.54      0.62        81\n",
      "           2       0.82      0.70      0.75        83\n",
      "           3       0.90      0.80      0.84       210\n",
      "           4       0.83      0.93      0.88       150\n",
      "           5       0.89      0.96      0.92       215\n",
      "           6       0.91      0.91      0.91       267\n",
      "           7       0.98      0.97      0.97       265\n",
      "           8       0.90      0.92      0.91       248\n",
      "           9       0.91      0.90      0.90       181\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.87      0.85      0.86      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "test_acc, preds, labels = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved a test accuracy of 88.3%, indicating fairly strong overall performance across the 10 topics. Most classes show balanced precision, recall, and F1-scores around 0.85–0.95, suggesting consistent prediction quality. However, classes 1 and 2 have noticeably lower recall (0.54 and 0.70), meaning the model sometimes fails to correctly identify samples from these categories. Despite that, the macro average F1-score of 0.86 confirms good generalization, and the weighted average F1-score of 0.88 shows that performance is reliable even considering class imbalance. Overall, the classifier performs well, with room for improvement in the minority or harder-to-distinguish classes."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
